# METAGEN-RecSys

# üõ†Ô∏è METAGEN-RS Benchmark
**Metadata-Conditioned Synthetic Reviews for Benchmarking Recommender Systems**
 
---
 
This repository contains two complete, modular scripts for creating and evaluating recommender systems using **synthetic text generation** (LLM-driven), extensive quality control, and comprehensive benchmarking.
 
- **`augment_pipeline.py`** ‚Üí Synthetic review generation & augmentation.
- **`recsys_pipeline.py`** ‚Üí Recommender system training, evaluation & hybrid ensembles.
 
---
 
## üöÄ Table of Contents
- [Overview](#overview)
- [Installation](#installation)
- [Detailed Workflow: Augmentation Pipeline](#detailed-workflow-augmentation-pipeline)
- [Detailed Workflow: Recommender Pipeline](#detailed-workflow-recommender-pipeline)
- [Outputs](#outputs)
- [Examples](#examples)
- [Extending the Benchmark](#extending-the-benchmark)
- [Citing](#citing)
 
---
 
## üéØ Overview
 
This project facilitates rigorous evaluation of recommender systems (RecSys) that utilize synthetic textual reviews generated by Large Language Models (LLMs). We provide complete data pipelines for:
 
- Generating realistic synthetic product reviews conditioned solely on item metadata.
- Systematically assessing synthetic review quality through perplexity, toxicity, and sentiment analysis.
- Training and evaluating state-of-the-art RecSys models across multiple dimensions:
    - **Accuracy**: Recall, NDCG, MRR.
    - **Beyond-Accuracy**: Novelty, Coverage, Cold-Start, Diversity, Calibration, and Fairness.
- Conducting hybrid model ensembles to combine strengths across models.
 
---
 
## üîß Installation
 
Clone the repository:
 
```bash
git clone https://github.com/your-org/metagen-rs-bench.git
cd metagen-rs-bench
```
 
Install dependencies:
 
```bash
pip install -r requirements.txt
```
 
Activate your OpenAI API key (needed for real LLM synthesis):
 
```bash
export OPENAI_API_KEY='your-openai-key-here'
```
 
---
 
## üìö Detailed Workflow: Augmentation Pipeline (`augment_pipeline.py`)
 
This script generates synthetic reviews for items from publicly available datasets (Amazon-2023 and MovieLens), performs quality assessment, and outputs ready-to-use augmented data.
 
### Key Stages:
 
### 1. **Dataset Acquisition & Harmonization**
- **Download raw data** (ratings and metadata):
  - Amazon (34 categories) or MovieLens (100K or 1M).
- **Harmonize IDs**:
  - Ensures consistent user/item IDs across ratings and metadata.
- **Aggregate user reviews**:
  - Optional aggregation of real user-generated reviews (for comparison purposes).
- **k-core filtering**:
  - Removes users/items below interaction threshold (default k=4).
 
### 2. **Synthetic Text Generation (LLM-driven)**
- Generates two types of synthetic reviews **per item**:
  - **Dense Summaries**: paragraph-style persuasive text.
  - **Aspect-Based Reviews**: structured bullet lists (features, pros, cons).
- Each review type is produced in three lengths:
  - **Short (‚âà50 tokens)**, **Medium (‚âà120 tokens)**, **Long (‚âà250 tokens)**.
- Utilizes GPT-4o-mini (default) with local caching to minimize API usage.
 
### 3. **Quality Assessment ("Critic")**
- Evaluates synthetic reviews on three metrics:
  - **Perplexity** (textual coherence via GPT-2).
  - **Toxicity** (via Detoxify).
  - **Sentiment Analysis** (via NLTK-VADER).
- Flags reviews that fail quality thresholds (configurable).
 
### 4. **Artifact Generation**
- Produces comprehensive datasets for immediate RecSys use:
  - `side_all.csv.gz`: augmented metadata, synthetic reviews.
  - `interactions.csv.gz`: user-item interactions.
  - `quality_metrics.csv`: detailed text-quality scores.
  - `manifest.json`: summary configuration & statistics.
 
---
 
## üéØ Detailed Workflow: Recommender Pipeline (`recsys_pipeline.py`)
 
This script trains, evaluates, and benchmarks recommender models leveraging augmented synthetic textual data.
 
### Key Stages:
 
### 1. **Data Loading & Splitting**
- Loads synthetic-enriched data (from local or remote).
- Supports splits:
  - **Temporal** (real-world scenario).
  - **Random stratified** (standard ML benchmarking).
 
### 2. **Model Families & Hyperparameter Optimization (HPO)**
 
#### Classical CF Models:
- **Matrix Factorization (MF)**
- **Bayesian Personalized Ranking (BPR)**
- **Variational Autoencoder CF (VAECF)**
 
#### Text-aware Models:
- **Collaborative Topic Regression (CTR)**
- **Collaborative Deep Learning (CDL)**
- **Hidden Factors and Topics (HFT)**
- **Convolutional Matrix Factorization (ConvMF)**
 
#### Dense (Image/Embeddings) Models:
- **Visual Bayesian Personalized Ranking (VBPR)**
- **Visual Matrix Factorization (VMF)**
- **Attribute-aware Matrix Factorization (AMR)** *(optional)*
 
- Conducts comprehensive **grid-search based HPO** for each model class.
 
### 3. **Recommendation Generation & Metrics**
- **Per-user Top-N recommendations**.
- Computes metrics at standard cut-offs (5, 10, 50):
  - **Accuracy**: Recall, NDCG, MRR.
  - **Beyond Accuracy**: Cold-start, Catalog coverage, Novelty, Popularity bias, Diversity, Fairness, Calibration bias.
 
### 4. **Hybrid Ensemble Benchmarking**
- Implements **fusion strategies** (Borda, Weighted Mean, Reciprocal Rank Fusion).
- Exhaustively benchmarks ensembles to identify best-performing hybrid strategies.
 
---
 
## üìÅ Outputs
 
| File | Description |
|------|-------------|
| `side_all.csv.gz` | Complete augmented synthetic metadata. |
| `interactions.csv.gz` | Filtered user-item interaction table. |
| `quality_metrics.csv` | Synthetic review quality scores. |
| `manifest.json` | Configuration snapshot. |
| `reclist_df_<slug>.csv` | Detailed per-user recommendations. |
| `agg_metrics_<slug>.csv` | Aggregated accuracy/beyond-accuracy metrics. |
| `ensemble_metrics_full_<slug>.csv` | Full ensemble evaluation metrics. |
| `metric_vs_nreviews_filtered_<slug>.pdf` | Metrics vs. #reviews plots. |
| `config_used_<slug>.json` | Full reproducibility configuration. |
 
---
 
## üìå Examples
 
Quick augmentation and RecSys benchmarking (using Amazon Fashion):
 
```bash
# Augmentation pipeline (small sample, quick run)
python augment_pipeline.py --ds_index 1 --fraction 0.05 --k_core 4
 
# RecSys training & evaluation pipeline
python recsys_pipeline.py --ds_index 1 --k_core 4 --split_mode temporal --hpo_preset small
```
 
---
 
## üîé Extending the Benchmark
 
- **Different LLMs**:
  - Replace default GPT-4o-mini with open-source LLMs like Mixtral.
- **Additional Metrics**:
  - Extend with fairness & bias metrics (the CSVs already include necessary data).
- **Additional RecSys Models**:
  - Integrate additional Cornac-compatible models seamlessly.
 
---
 
## üìñ Citing
 
If you use METAGEN-RS, please cite:
 

```
 
---
 
## ‚öôÔ∏è Reproducibility & Transparency
 
| Aspect | Approach |
|--------|----------|
| Random seeds | Set globally (`CFG.seed = 42`) |
| Data splits | Fully deterministic with fixed seed |
| LLM outputs | Cached locally via `aug_cache/` |
| Environment | Pinned versions in `requirements.txt` |
 
---
 
üéâ **Happy Benchmarking!**
 
> **Questions or feedback?**  
> Open an issue or PR‚Äîwe appreciate your contributions!
